{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8X1xE2QFowaZE9rCyvcoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akanksha-prajapati/Deep-learning/blob/main/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvS53nxnileB"
      },
      "outputs": [],
      "source": [
        "# How do you install and verify that TensorFlow 2.0 was installed successfully\n",
        "\n",
        "#code...\n",
        "\n",
        "#Step 1: Install TensorFlow 2.0\n",
        "pip install tensorflow==2.0\n",
        "pip install tensorflow\n",
        "\n",
        "#Step 2: Verify the Installation\n",
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Verify if TensorFlow is working\n",
        "print(\"Is TensorFlow built with CUDA?\", tf.test.is_built_with_cuda())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How can you define a simple function in TensorFlow 2.0 to perform addition ?\n",
        "\n",
        "#code...\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a function that adds two numbers (or tensors)\n",
        "def add_two_numbers(a, b):\n",
        "    return tf.add(a, b)\n",
        "\n",
        "# Test the function\n",
        "x = tf.constant(5)\n",
        "y = tf.constant(3)\n",
        "result = add_two_numbers(x, y)\n",
        "\n",
        "print(\"Result of addition:\", result.numpy())  # .numpy() is used to get the value from the tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "0_pWvTAUkN-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How can you create a simple neural network in TensorFlow 2.0 with one hidden layer ?\n",
        "\n",
        "#code...\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Input layer (implicitly added with the first hidden layer)\n",
        "# Hidden layer with 10 neurons and ReLU activation function\n",
        "model.add(layers.Dense(10, activation='relu', input_shape=(4,)))  # Example input shape (4 features)\n",
        "\n",
        "# Output layer with a single neuron (for binary classification, for example)\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "hSKa_zF3koqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  How can you visualize the training progress using TensorFlow and Matplotlib ?\n",
        "\n",
        "#code...\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the model (as previously defined)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Example random data (100 samples with 4 features each)\n",
        "X_train = np.random.rand(100, 4)\n",
        "y_train = np.random.randint(0, 2, size=(100, 1))  # Binary target values (0 or 1)\n",
        "\n",
        "# Train the model and save the history\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Visualize the training progress\n",
        "# Get the history of loss and accuracy\n",
        "loss = history.history['loss']\n",
        "accuracy = history.history['accuracy']\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot loss\n",
        "ax1.plot(loss, label='Training Loss', color='blue')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "# Plot accuracy\n",
        "ax2.plot(accuracy, label='Training Accuracy', color='green')\n",
        "ax2.set_title('Training Accuracy')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mVmQLzmNk7UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you install PyTorch and verify the PyTorch installation ??\n",
        "\n",
        "#code..\n",
        "\n",
        "#Step 1: Install PyTorch\n",
        "pip install torch torchvision torchaudio\n",
        "#Step 2: Verify PyTorch Installation\n",
        "import torch\n",
        "\n",
        "# Check PyTorch version\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# Test a simple tensor operation\n",
        "x = torch.rand(5, 3)\n",
        "print(\"Random tensor:\", x)\n",
        "\n"
      ],
      "metadata": {
        "id": "dApkEqdBlXbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you create a simple neural network in PyTorch ?\n",
        "\n",
        "#code...\n",
        " import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        # Define the layers\n",
        "        self.hidden = nn.Linear(4, 10)  # Input layer: 4 features, hidden layer: 10 neurons\n",
        "        self.output = nn.Linear(10, 1)   # Output layer: 1 neuron (for binary classification)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the hidden layer with ReLU activation\n",
        "        x = torch.relu(self.hidden(x))\n",
        "\n",
        "        # Forward pass through the output layer with sigmoid activation (for binary classification)\n",
        "        x = torch.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "loss_fn = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example random data (100 samples with 4 features each)\n",
        "X_train = torch.rand(100, 4)\n",
        "y_train = torch.randint(0, 2, (100, 1)).float()  # Random binary labels\n",
        "\n",
        "# Training the model\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_fn(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()        # Compute gradients\n",
        "    optimizer.step()       # Update weights\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xDcDtfPUm0K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you define a loss function and optimizer in PyTorch\n",
        "\n",
        "#code...\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden = nn.Linear(4, 10)  # Hidden layer with 10 neurons\n",
        "        self.output = nn.Linear(10, 1)  # Output layer with 1 neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden(x))\n",
        "        x = torch.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.BCELoss()  # For binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example random data\n",
        "X_train = torch.rand(100, 4)  # 100 samples, 4 features\n",
        "y_train = torch.randint(0, 2, (100, 1)).float()  # Binary labels (0 or 1)\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Clear gradients from the previous step\n",
        "    output = model(X_train)  # Forward pass\n",
        "    loss = loss_fn(output, y_train)  # Calculate loss\n",
        "    loss.backward()  # Backpropagate the gradients\n",
        "    optimizer.step()  # Update the model parameters\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "bff1ZmZtno6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do you implement a custom loss function in PyTorch ?\n",
        "\n",
        "#code...\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Custom Loss Class (Subclassing nn.Module)\n",
        "class CustomMSLELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMSLELoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Calculate MSLE (Mean Squared Logarithmic Error)\n",
        "        log_pred = torch.log(y_pred + 1)  # Apply log(x + 1) to predicted values\n",
        "        log_true = torch.log(y_true + 1)  # Apply log(x + 1) to true values\n",
        "        loss = torch.mean((log_pred - log_true) ** 2)  # Calculate the mean squared difference\n",
        "        return loss\n",
        "\n",
        "# Example Usage\n",
        "loss_fn = CustomMSLELoss()\n",
        "\n",
        "# Example data\n",
        "y_pred = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)  # Predicted values\n",
        "y_true = torch.tensor([1.1, 1.9, 3.0, 3.8])  # True values\n",
        "\n",
        "# Compute loss\n",
        "loss = loss_fn(y_pred, y_true)\n",
        "print(f\"Custom MSLE Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "1duC3jl8oIRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}